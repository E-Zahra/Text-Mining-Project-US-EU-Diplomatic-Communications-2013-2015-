{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertopic\n",
      "  Downloading bertopic-0.17.4-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.2.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting umap-learn\n",
      "  Downloading umap_learn-0.5.9.post2-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting hdbscan\n",
      "  Downloading hdbscan-0.8.41-cp311-cp311-macosx_10_9_universal2.whl.metadata (15 kB)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from bertopic) (2.2.5)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from bertopic) (2.2.3)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from bertopic) (6.5.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from bertopic) (1.7.2)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /Users/majid/.local/lib/python3.11/site-packages (from bertopic) (4.67.1)\n",
      "Collecting llvmlite>0.36.0 (from bertopic)\n",
      "  Downloading llvmlite-0.46.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from sentence-transformers) (4.57.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from sentence-transformers) (1.16.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/majid/.local/lib/python3.11/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Collecting numba>=0.51.2 (from umap-learn)\n",
      "  Downloading numba-0.63.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.9 kB)\n",
      "Collecting pynndescent>=0.5 (from umap-learn)\n",
      "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: joblib>=1.0 in /Users/majid/.local/lib/python3.11/site-packages (from hdbscan) (1.5.2)\n",
      "Collecting smart_open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from plotly>=4.7.0->bertopic) (2.14.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from scikit-learn>=1.0->bertopic) (3.6.0)\n",
      "Collecting wrapt (from smart_open>=1.8.1->gensim)\n",
      "  Downloading wrapt-2.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/myenv/lib/python3.11/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.1.31)\n",
      "Downloading bertopic-0.17.4-py3-none-any.whl (154 kB)\n",
      "Downloading sentence_transformers-5.2.0-py3-none-any.whl (493 kB)\n",
      "Downloading umap_learn-0.5.9.post2-py3-none-any.whl (90 kB)\n",
      "Downloading hdbscan-0.8.41-cp311-cp311-macosx_10_9_universal2.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gensim-4.4.0-cp311-cp311-macosx_11_0_arm64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m  \u001b[33m0:00:11\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.46.0-cp311-cp311-macosx_11_0_arm64.whl (37.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.2/37.2 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m  \u001b[33m0:00:10\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numba-0.63.1-cp311-cp311-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Downloading wrapt-2.0.1-cp311-cp311-macosx_11_0_arm64.whl (61 kB)\n",
      "Installing collected packages: wrapt, llvmlite, smart_open, numba, pynndescent, hdbscan, gensim, umap-learn, sentence-transformers, bertopic\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/10\u001b[0m [bertopic]/10\u001b[0m [bertopic]transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed bertopic-0.17.4 gensim-4.4.0 hdbscan-0.8.41 llvmlite-0.46.0 numba-0.63.1 pynndescent-0.5.13 sentence-transformers-5.2.0 smart_open-7.5.0 umap-learn-0.5.9.post2 wrapt-2.0.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install bertopic sentence-transformers umap-learn hdbscan gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_DIRS = [\"2014\", \"2015\"]   # or one folder if you prefer\n",
    "OUT_DIR = \"outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define EU list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EU_COUNTRIES = {\n",
    "    \"Austria\",\"Belgium\",\"Bulgaria\",\"Croatia\",\"Cyprus\",\"Czech Republic\",\"Denmark\",\n",
    "    \"Estonia\",\"Finland\",\"France\",\"Germany\",\"Greece\",\"Hungary\",\"Ireland\",\"Italy\",\n",
    "    \"Latvia\",\"Lithuania\",\"Luxembourg\",\"Malta\",\"Netherlands\",\"Poland\",\"Portugal\",\n",
    "    \"Romania\",\"Spain\",\"Sweden\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(390, ['2014/Afghanistan.txt', '2014/Albania.txt', '2014/Algeria.txt'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = []\n",
    "for d in DATA_DIRS:\n",
    "    paths.extend(glob.glob(os.path.join(d, \"*.txt\")))\n",
    "paths = sorted(paths)\n",
    "\n",
    "len(paths), paths[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning + section split + chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_STYLE_RE = re.compile(r\"<(script|style)[^>]*>.*?</\\1>\", re.IGNORECASE | re.DOTALL)\n",
    "TAG_RE = re.compile(r\"<[^>]+>\")\n",
    "WS_RE = re.compile(r\"\\s+\")\n",
    "\n",
    "def strip_html(html: str) -> str:\n",
    "    html = SCRIPT_STYLE_RE.sub(\" \", html)\n",
    "    text = TAG_RE.sub(\" \", html)\n",
    "    text = (text.replace(\"&amp;\", \"&\")\n",
    "                .replace(\"&nbsp;\", \" \")\n",
    "                .replace(\"&quot;\", '\"')\n",
    "                .replace(\"&lt;\", \"<\")\n",
    "                .replace(\"&gt;\", \">\"))\n",
    "    return WS_RE.sub(\" \", text).strip()\n",
    "\n",
    "def detect_sections(text: str):\n",
    "    headings = [\n",
    "        \"EXECUTIVE SUMMARY\", \"EXECUTIVE SUMMARY:\",\n",
    "        \"Section 1.\", \"SECTION 1.\", \"Section 1:\", \"SECTION 1:\",\n",
    "        \"Section 2.\", \"SECTION 2.\", \"Section 2:\", \"SECTION 2:\",\n",
    "        \"Section 3.\", \"SECTION 3.\", \"Section 3:\", \"SECTION 3:\",\n",
    "        \"Section 4.\", \"SECTION 4.\", \"Section 4:\", \"SECTION 4:\",\n",
    "        \"Section 5.\", \"SECTION 5.\", \"Section 5:\", \"SECTION 5:\",\n",
    "        \"Section 6.\", \"SECTION 6.\", \"Section 6:\", \"SECTION 6:\",\n",
    "        \"Section 7.\", \"SECTION 7.\", \"Section 7:\", \"SECTION 7:\",\n",
    "    ]\n",
    "    pattern = \"(\" + \"|\".join(re.escape(h) for h in headings) + \")\"\n",
    "    parts = re.split(pattern, text)\n",
    "\n",
    "    if len(parts) <= 1:\n",
    "        return [(\"FULL_TEXT\", text)]\n",
    "\n",
    "    sections = []\n",
    "    pre = parts[0].strip()\n",
    "    if pre:\n",
    "        sections.append((\"PREAMBLE\", pre))\n",
    "\n",
    "    i = 1\n",
    "    while i < len(parts) - 1:\n",
    "        title = parts[i].strip()\n",
    "        body = parts[i+1].strip()\n",
    "        if body:\n",
    "            sections.append((title, body))\n",
    "        i += 2\n",
    "    return sections\n",
    "\n",
    "def split_into_word_chunks(text: str, min_words=120, max_words=250):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        j = min(i + max_words, len(words))\n",
    "        chunk = words[i:j]\n",
    "        if len(chunk) >= min_words:\n",
    "            chunks.append(\" \".join(chunk))\n",
    "        i = j\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the full dataset table (ALL countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18052, 7),\n",
       "    year      country  is_eu            section      source_file  \\\n",
       " 0  2014  Afghanistan  False  EXECUTIVE SUMMARY  Afghanistan.txt   \n",
       " 1  2014  Afghanistan  False  EXECUTIVE SUMMARY  Afghanistan.txt   \n",
       " 2  2014  Afghanistan  False         Section 1.  Afghanistan.txt   \n",
       " 3  2014  Afghanistan  False         Section 1.  Afghanistan.txt   \n",
       " 4  2014  Afghanistan  False         Section 1.  Afghanistan.txt   \n",
       " \n",
       "                                 chunk_id  \\\n",
       " 0  Afghanistan.txt::EXECUTIVE SUMMARY::0   \n",
       " 1  Afghanistan.txt::EXECUTIVE SUMMARY::1   \n",
       " 2         Afghanistan.txt::Section 1.::0   \n",
       " 3         Afghanistan.txt::Section 1.::1   \n",
       " 4         Afghanistan.txt::Section 1.::2   \n",
       " \n",
       "                                                 text  \n",
       " 0  Share Afghanistan is an Islamic republic with ...  \n",
       " 1  detention; judicial corruption and ineffective...  \n",
       " 2  Respect for the Integrity of the Person, Inclu...  \n",
       " 3  compared with the same period in 2013. The tot...  \n",
       " 4  in Paktika Province killed 45 civilians and in...  )"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "for p in paths:\n",
    "    with open(p, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        year = f.readline().strip()\n",
    "        country = f.readline().strip()\n",
    "        html = f.read()\n",
    "\n",
    "    plain = strip_html(html)\n",
    "    for sec_title, sec_text in detect_sections(plain):\n",
    "        for k, ch in enumerate(split_into_word_chunks(sec_text, 120, 250)):\n",
    "            rows.append({\n",
    "                \"year\": year,\n",
    "                \"country\": country,\n",
    "                \"is_eu\": country in EU_COUNTRIES,\n",
    "                \"section\": sec_title,\n",
    "                \"source_file\": os.path.basename(p),\n",
    "                \"chunk_id\": f\"{os.path.basename(p)}::{sec_title}::{k}\",\n",
    "                \"text\": ch\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.shape, df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify counts (world vs EU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(year\n",
       " 2014    195\n",
       " 2015    195\n",
       " Name: country, dtype: int64,\n",
       " np.float64(0.1028140926213162))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"year\"])[\"country\"].nunique(), df[\"is_eu\"].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "2014    25\n",
       "2015    25\n",
       "Name: country, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many EU chunks?\n",
    "df[df[\"is_eu\"]].groupby(\"year\")[\"country\"].nunique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit BERTopic on the full corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-01 15:04:13,948 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 565/565 [03:30<00:00,  2.69it/s]\n",
      "2026-01-01 15:07:45,206 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-01 15:07:45,207 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-01 15:08:03,273 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-01 15:08:03,274 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2026-01-01 15:08:06,191 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-01 15:08:06,200 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-01 15:08:08,106 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>5557</td>\n",
       "      <td>-1_the_of_to_and</td>\n",
       "      <td>[the, of, to, and, in, for, were, on, or, that]</td>\n",
       "      <td>[90 days while the PIC continues its investiga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2940</td>\n",
       "      <td>0_labor_workers_work_union</td>\n",
       "      <td>[labor, workers, work, union, unions, employer...</td>\n",
       "      <td>[Worker Rights Share a. Freedom of Association...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1041</td>\n",
       "      <td>1_elections_political_election_parties</td>\n",
       "      <td>[elections, political, election, parties, part...</td>\n",
       "      <td>[Respect for Political Rights: The Right of Ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>816</td>\n",
       "      <td>2_corruption_officials_public_information</td>\n",
       "      <td>[corruption, officials, public, information, d...</td>\n",
       "      <td>[Corruption and Lack of Transparency in Govern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>554</td>\n",
       "      <td>3_human_rights_international_ngos</td>\n",
       "      <td>[human, rights, international, ngos, bodies, g...</td>\n",
       "      <td>[Governmental Attitude Regarding International...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>384</td>\n",
       "      <td>4_disabilities_with_persons_mental</td>\n",
       "      <td>[disabilities, with, persons, mental, ethnic, ...</td>\n",
       "      <td>[While the government effectively enforced the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>365</td>\n",
       "      <td>5_rape_violence_domestic_spousal</td>\n",
       "      <td>[rape, violence, domestic, spousal, women, dis...</td>\n",
       "      <td>[Discrimination, Societal Abuses, and Traffick...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>282</td>\n",
       "      <td>6_police_security_forces_arrest</td>\n",
       "      <td>[police, security, forces, arrest, responsible...</td>\n",
       "      <td>[observers. While the national Red Cross and c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>258</td>\n",
       "      <td>7_prisoners_prison_prisons_monitoring</td>\n",
       "      <td>[prisoners, prison, prisons, monitoring, compl...</td>\n",
       "      <td>[Prison administrators did not maintain record...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>207</td>\n",
       "      <td>8_internet_websites_freedom_content</td>\n",
       "      <td>[internet, websites, freedom, content, blocked...</td>\n",
       "      <td>[National Forum for the Democratization of Com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                       Name  \\\n",
       "0     -1   5557                           -1_the_of_to_and   \n",
       "1      0   2940                 0_labor_workers_work_union   \n",
       "2      1   1041     1_elections_political_election_parties   \n",
       "3      2    816  2_corruption_officials_public_information   \n",
       "4      3    554          3_human_rights_international_ngos   \n",
       "5      4    384         4_disabilities_with_persons_mental   \n",
       "6      5    365           5_rape_violence_domestic_spousal   \n",
       "7      6    282            6_police_security_forces_arrest   \n",
       "8      7    258      7_prisoners_prison_prisons_monitoring   \n",
       "9      8    207        8_internet_websites_freedom_content   \n",
       "\n",
       "                                      Representation  \\\n",
       "0    [the, of, to, and, in, for, were, on, or, that]   \n",
       "1  [labor, workers, work, union, unions, employer...   \n",
       "2  [elections, political, election, parties, part...   \n",
       "3  [corruption, officials, public, information, d...   \n",
       "4  [human, rights, international, ngos, bodies, g...   \n",
       "5  [disabilities, with, persons, mental, ethnic, ...   \n",
       "6  [rape, violence, domestic, spousal, women, dis...   \n",
       "7  [police, security, forces, arrest, responsible...   \n",
       "8  [prisoners, prison, prisons, monitoring, compl...   \n",
       "9  [internet, websites, freedom, content, blocked...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [90 days while the PIC continues its investiga...  \n",
       "1  [Worker Rights Share a. Freedom of Association...  \n",
       "2  [Respect for Political Rights: The Right of Ci...  \n",
       "3  [Corruption and Lack of Transparency in Govern...  \n",
       "4  [Governmental Attitude Regarding International...  \n",
       "5  [While the government effectively enforced the...  \n",
       "6  [Discrimination, Societal Abuses, and Traffick...  \n",
       "7  [observers. While the national Red Cross and c...  \n",
       "8  [Prison administrators did not maintain record...  \n",
       "9  [National Forum for the Democratization of Com...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "docs_all = df[\"text\"].tolist()\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedder,\n",
    "    calculate_probabilities=False,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "topics, _ = topic_model.fit_transform(docs_all)\n",
    "df[\"topic\"] = topics\n",
    "\n",
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper to view topic words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['labor', 'workers', 'work', 'union', 'unions', 'employers', 'employment', 'sector', 'minimum', 'child']\n",
      "1 ['elections', 'political', 'election', 'parties', 'participation', 'party', 'seats', 'fair', 'elected', 'free']\n",
      "2 ['corruption', 'officials', 'public', 'information', 'disclosure', 'financial', 'transparency', 'government', 'assets', 'anticorruption']\n",
      "3 ['human', 'rights', 'international', 'ngos', 'bodies', 'government', 'attitude', 'governmental', 'ombudsman', 'organizations']\n",
      "4 ['disabilities', 'with', 'persons', 'mental', 'ethnic', 'buildings', 'education', 'schools', 'accessible', 'disability']\n",
      "5 ['rape', 'violence', 'domestic', 'spousal', 'women', 'discrimination', 'societal', 'gender', 'sexual', 'race']\n",
      "6 ['police', 'security', 'forces', 'arrest', 'responsible', 'apparatus', 'internal', 'role', 'force', 'ministry']\n",
      "7 ['prisoners', 'prison', 'prisons', 'monitoring', 'complaints', 'detention', 'visits', 'permitted', 'conditions', 'independent']\n"
     ]
    }
   ],
   "source": [
    "def get_topic_words(model, topic_id, topn=10):\n",
    "    pairs = model.get_topic(topic_id) or []\n",
    "    return [w for w, _ in pairs[:topn]]\n",
    "\n",
    "valid_topic_ids = [t for t in topic_model.get_topics().keys() if t != -1]\n",
    "for tid in valid_topic_ids[:8]:\n",
    "    print(tid, get_topic_words(topic_model, tid, 10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (do it on ALL, then interpret EU)\n",
    "### Coherence + diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'coherence_c_v': 0.7004116820920762,\n",
       " 'topic_diversity_top10': 0.5082758620689655,\n",
       " 'n_topics_excluding_outliers': 145}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def topic_diversity(topic_words, topk=10):\n",
    "    all_words = []\n",
    "    for wlist in topic_words:\n",
    "        all_words.extend(wlist[:topk])\n",
    "    return len(set(all_words)) / max(1, len(all_words))\n",
    "\n",
    "def coherence_cv(tokenized_docs, topic_words):\n",
    "    dictionary = Dictionary(tokenized_docs)\n",
    "    corpus = [dictionary.doc2bow(toks) for toks in tokenized_docs]\n",
    "    cm = CoherenceModel(\n",
    "        topics=topic_words,\n",
    "        texts=tokenized_docs,\n",
    "        corpus=corpus,\n",
    "        dictionary=dictionary,\n",
    "        coherence=\"c_v\"\n",
    "    )\n",
    "    return float(cm.get_coherence())\n",
    "\n",
    "topic_words = [get_topic_words(topic_model, tid, 20) for tid in valid_topic_ids]\n",
    "tokenized = [d.split() for d in docs_all]\n",
    "\n",
    "metrics = {\n",
    "    \"coherence_c_v\": coherence_cv(tokenized, topic_words),\n",
    "    \"topic_diversity_top10\": topic_diversity(topic_words, topk=10),\n",
    "    \"n_topics_excluding_outliers\": len(valid_topic_ids)\n",
    "}\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability (quick but defendable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'stability_mean_best_jaccard': 0.7828282828282828,\n",
       " 'stability_median_best_jaccard': 0.8181818181818182}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jaccard(a, b):\n",
    "    a, b = set(a), set(b)\n",
    "    return len(a & b) / max(1, len(a | b))\n",
    "\n",
    "def run_model_with_seed(seed: int):\n",
    "    np.random.seed(seed)\n",
    "    idx = np.random.permutation(len(docs_all))\n",
    "    docs_shuffled = [docs_all[i] for i in idx]\n",
    "    tm = BERTopic(embedding_model=embedder, verbose=False)\n",
    "    tm.fit_transform(docs_shuffled)\n",
    "    valid = [tid for tid in tm.get_topics().keys() if tid != -1]\n",
    "    return {tid: get_topic_words(tm, tid, 15) for tid in valid}\n",
    "\n",
    "w1 = run_model_with_seed(1)\n",
    "w2 = run_model_with_seed(2)\n",
    "\n",
    "scores = []\n",
    "for t1, words1 in list(w1.items())[:15]:\n",
    "    best = 0.0\n",
    "    for t2, words2 in w2.items():\n",
    "        best = max(best, jaccard(words1, words2))\n",
    "    scores.append(best)\n",
    "\n",
    "{\"stability_mean_best_jaccard\": float(np.mean(scores)),\n",
    " \"stability_median_best_jaccard\": float(np.median(scores))}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EU-focused interpretation (the “easy to interpret” part)\n",
    "### Topic share inside EU vs non-EU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>group</th>\n",
       "      <th>topic</th>\n",
       "      <th>EU</th>\n",
       "      <th>Non-EU</th>\n",
       "      <th>delta_EU_minus_NonEU</th>\n",
       "      <th>top_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.059265</td>\n",
       "      <td>0.004032</td>\n",
       "      <td>0.055233</td>\n",
       "      <td>roma, romani, housing, school, education, scho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.045761</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>0.042088</td>\n",
       "      <td>anti, semitic, jewish, semitism, holocaust, je...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>0.030758</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.030041</td>\n",
       "      <td>asylum, safe, seekers, eu, transit, countries,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.024756</td>\n",
       "      <td>0.004032</td>\n",
       "      <td>0.020725</td>\n",
       "      <td>speech, press, freedom, expression, liberties,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>0.016504</td>\n",
       "      <td>0.002598</td>\n",
       "      <td>0.013906</td>\n",
       "      <td>echr, european, decisions, remedies, appeal, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>92</td>\n",
       "      <td>0.012753</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.012216</td>\n",
       "      <td>racism, racist, discrimination, racial, hate, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>0.015004</td>\n",
       "      <td>0.004032</td>\n",
       "      <td>0.010972</td>\n",
       "      <td>stateless, citizenship, persons, unhcr, statel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>0.010503</td>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.009248</td>\n",
       "      <td>trial, fair, right, defendants, judiciary, ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.027757</td>\n",
       "      <td>0.019799</td>\n",
       "      <td>0.007958</td>\n",
       "      <td>prisoners, prison, prisons, monitoring, compla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.037509</td>\n",
       "      <td>0.029923</td>\n",
       "      <td>0.007586</td>\n",
       "      <td>disabilities, with, persons, mental, ethnic, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>86</td>\n",
       "      <td>0.008252</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.007087</td>\n",
       "      <td>restitution, property, claims, echr, compensat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.035259</td>\n",
       "      <td>0.028490</td>\n",
       "      <td>0.006769</td>\n",
       "      <td>rape, violence, domestic, spousal, women, disc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>119</td>\n",
       "      <td>0.006752</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.006393</td>\n",
       "      <td>asylum, seekers, deportation, detention, rejec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59</td>\n",
       "      <td>0.008252</td>\n",
       "      <td>0.002240</td>\n",
       "      <td>0.006012</td>\n",
       "      <td>lgbt, parade, hiv, pride, lgbti, aids, event, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>0.007502</td>\n",
       "      <td>0.001881</td>\n",
       "      <td>0.005620</td>\n",
       "      <td>freedom, refugees, academic, protection, movem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "group  topic        EU    Non-EU  delta_EU_minus_NonEU  \\\n",
       "14        14  0.059265  0.004032              0.055233   \n",
       "21        21  0.045761  0.003673              0.042088   \n",
       "46        46  0.030758  0.000717              0.030041   \n",
       "26        26  0.024756  0.004032              0.020725   \n",
       "43        43  0.016504  0.002598              0.013906   \n",
       "92        92  0.012753  0.000538              0.012216   \n",
       "34        34  0.015004  0.004032              0.010972   \n",
       "76        76  0.010503  0.001254              0.009248   \n",
       "7          7  0.027757  0.019799              0.007958   \n",
       "4          4  0.037509  0.029923              0.007586   \n",
       "86        86  0.008252  0.001165              0.007087   \n",
       "5          5  0.035259  0.028490              0.006769   \n",
       "119      119  0.006752  0.000358              0.006393   \n",
       "59        59  0.008252  0.002240              0.006012   \n",
       "71        71  0.007502  0.001881              0.005620   \n",
       "\n",
       "group                                          top_words  \n",
       "14     roma, romani, housing, school, education, scho...  \n",
       "21     anti, semitic, jewish, semitism, holocaust, je...  \n",
       "46     asylum, safe, seekers, eu, transit, countries,...  \n",
       "26     speech, press, freedom, expression, liberties,...  \n",
       "43     echr, european, decisions, remedies, appeal, h...  \n",
       "92     racism, racist, discrimination, racial, hate, ...  \n",
       "34     stateless, citizenship, persons, unhcr, statel...  \n",
       "76     trial, fair, right, defendants, judiciary, ind...  \n",
       "7      prisoners, prison, prisons, monitoring, compla...  \n",
       "4      disabilities, with, persons, mental, ethnic, b...  \n",
       "86     restitution, property, claims, echr, compensat...  \n",
       "5      rape, violence, domestic, spousal, women, disc...  \n",
       "119    asylum, seekers, deportation, detention, rejec...  \n",
       "59     lgbt, parade, hiv, pride, lgbti, aids, event, ...  \n",
       "71     freedom, refugees, academic, protection, movem...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def topic_share(subdf: pd.DataFrame, label: str):\n",
    "    t = (subdf[subdf[\"topic\"] != -1]\n",
    "         .groupby(\"topic\").size().reset_index(name=\"n\"))\n",
    "    t[\"share\"] = t[\"n\"] / t[\"n\"].sum()\n",
    "    t[\"group\"] = label\n",
    "    return t\n",
    "\n",
    "eu = topic_share(df[df[\"is_eu\"]], \"EU\")\n",
    "non_eu = topic_share(df[~df[\"is_eu\"]], \"Non-EU\")\n",
    "\n",
    "compare = pd.concat([eu, non_eu], ignore_index=True)\n",
    "compare = compare.pivot_table(index=\"topic\", columns=\"group\", values=\"share\", fill_value=0.0).reset_index()\n",
    "compare[\"delta_EU_minus_NonEU\"] = compare.get(\"EU\", 0.0) - compare.get(\"Non-EU\", 0.0)\n",
    "\n",
    "compare[\"top_words\"] = compare[\"topic\"].apply(lambda t: \", \".join(get_topic_words(topic_model, int(t), 8)))\n",
    "compare.sort_values(\"delta_EU_minus_NonEU\", ascending=False).head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change over time inside EU (2014 → 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_df = df[df[\"is_eu\"] & (df[\"topic\"] != -1)].copy()\n",
    "\n",
    "year_topic = (eu_df.groupby([\"year\", \"topic\"]).size().reset_index(name=\"n\"))\n",
    "year_topic[\"share_within_year\"] = year_topic[\"n\"] / year_topic.groupby(\"year\")[\"n\"].transform(\"sum\")\n",
    "\n",
    "pivot = year_topic.pivot_table(index=\"topic\", columns=\"year\", values=\"share_within_year\", fill_value=0.0)\n",
    "if len(pivot.columns) >= 2:\n",
    "    years = sorted(pivot.columns.tolist())\n",
    "    pivot[\"delta\"] = pivot[years[-1]] - pivot[years[0]]\n",
    "    out = pivot.sort_values(\"delta\", ascending=False).head(15).copy()\n",
    "    out[\"top_words\"] = out.index.map(lambda t: \", \".join(get_topic_words(topic_model, int(t), 8)))\n",
    "    out.reset_index()\n",
    "else:\n",
    "    print(\"Need both years present to compute change.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot labels on EU chunks (sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "zshot = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "LABELS = [\n",
    "    \"corruption\",\n",
    "    \"judicial independence\",\n",
    "    \"police abuse\",\n",
    "    \"prison conditions\",\n",
    "    \"freedom of expression\",\n",
    "    \"freedom of assembly\",\n",
    "    \"religious freedom\",\n",
    "    \"refugees and asylum\",\n",
    "    \"human trafficking\",\n",
    "    \"anti-Semitism\",\n",
    "    \"LGBTQ+ rights\",\n",
    "    \"women's rights\",\n",
    "    \"labor rights\"\n",
    "]\n",
    "\n",
    "zs = eu_df.sample(min(400, len(eu_df)), random_state=7).copy()\n",
    "res = zshot(zs[\"text\"].tolist(), candidate_labels=LABELS, multi_label=True)\n",
    "\n",
    "top3 = []\n",
    "for r in res:\n",
    "    pairs = sorted(zip(r[\"labels\"], r[\"scores\"]), key=lambda x: x[1], reverse=True)[:3]\n",
    "    top3.append([p[0] for p in pairs])\n",
    "\n",
    "zs[\"top3_labels\"] = top3\n",
    "labels_by_topic = (zs[[\"topic\",\"top3_labels\"]].explode(\"top3_labels\")\n",
    "                   .groupby([\"topic\",\"top3_labels\"]).size()\n",
    "                   .reset_index(name=\"count\")\n",
    "                   .sort_values([\"topic\",\"count\"], ascending=[True, False]))\n",
    "labels_by_topic.head(20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
